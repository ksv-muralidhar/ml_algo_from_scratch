{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a41273e-0d49-4ffe-9e10-130babfc3382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be5e16f-08d9-48e0-b845-ef684cccc69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchGradientDescentRegressor:\n",
    "    '''\n",
    "    Fits Batch Gradient Descent Regression model.\n",
    "    It is required to scale the input features before fitting the model. \n",
    "    \n",
    "    Params:\n",
    "    eta (float): Learning rate\n",
    "    epochs (int): number of epochs\n",
    "    tol (float): minimum reduction in loss required to continue training\n",
    "    '''\n",
    "    def __init__(self, eta: float = 0.01, epochs: int=1000, tol: float=0.001, random_state: int = 42):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.__z = None\n",
    "        self.__af = None\n",
    "        self.__errors = None\n",
    "        self.loss = 1\n",
    "        self.loss_history = [0]\n",
    "\n",
    "\n",
    "    def __initialise_weights(self, n_col: int): # random weight initialization\n",
    "        np.random.seed(self.random_state)\n",
    "        self.weights = np.random.randn(n_col)\n",
    "\n",
    "        \n",
    "    def __sum_function(self, x: np.array):\n",
    "        self.__z = np.dot(x, self.weights.reshape(-1, 1)) # X.W\n",
    "        self.__af = self.__z.ravel()\n",
    "\n",
    "        \n",
    "    def __weight_update(self, x: np.array, y: np.array): # update wwights\n",
    "        n_row = len(self.__af)\n",
    "        self.__errors = self.__af - (y).ravel() # y_hat - y\n",
    "        self.weights = self.weights - (self.eta * (1 / n_row) * np.dot(self.__errors, x))\n",
    "\n",
    "        \n",
    "    def __loss_update(self): # update loss\n",
    "        self.loss = np.mean(self.__errors ** 2)\n",
    "\n",
    "        \n",
    "    def fit(self, x: np.array, y: np.array):\n",
    "        n_row, _ = x.shape\n",
    "        ones = np.ones((n_row, 1)) # array of 1s\n",
    "        x = np.hstack((ones, x)) # appending array of 1s to x as bias feature\n",
    "        _, n_col = x.shape\n",
    "        self.__initialise_weights(n_col=n_col) # random weight initialization\n",
    "        epoch = 0\n",
    "\n",
    "        while epoch < self.epochs: \n",
    "            self.__sum_function(x=x) # prediction\n",
    "            self.__weight_update(x=x, y=y) # weight update\n",
    "            self.__loss_update() # update loss\n",
    "            epoch += 1\n",
    "            print(f'Epoch {epoch} / {self.epochs}\\tTraining MSE: {np.round(self.loss, 5)}')\n",
    "            self.loss_history.append(self.loss) # add loss to loss_history list\n",
    "            \n",
    "            if np.abs(self.loss_history[-1] - self.loss_history[-2]) < self.tol:\n",
    "                print(\"Exiting as reduction in MSE < tol\")\n",
    "                break\n",
    "        return self\n",
    "\n",
    "                \n",
    "    def predict(self, x: np.array):\n",
    "        n_row, _ = x.shape\n",
    "        ones = np.ones((n_row, 1)) # array of 1s\n",
    "        x = np.hstack((ones, x)) # appending array of 1s to x as bias feature\n",
    "        return np.dot(x, self.weights.reshape(-1, 1)) # prediction\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BatchGradientDescentRegressor(eta={self.eta}, epochs={self.epochs}, tot={self.tol}, random_state={self.random_state})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2184265d-ea94-4812-952e-ee1c1ccec735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fefb019-7c0f-441c-93ef-ec554e62de8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = pd.DataFrame({'x1': np.arange(100000), 'x2': np.arange(100000, 0, -1), 'x3': np.random.randint(low=0, high=100000, size=100000)})\n",
    "data['y'] = data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5193cc9f-75b0-4db5-bd2f-0811fd112542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>15795</td>\n",
       "      <td>115795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>860</td>\n",
       "      <td>100860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>99998</td>\n",
       "      <td>76820</td>\n",
       "      <td>176820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>99997</td>\n",
       "      <td>54886</td>\n",
       "      <td>154886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>99996</td>\n",
       "      <td>6265</td>\n",
       "      <td>106265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>5</td>\n",
       "      <td>57885</td>\n",
       "      <td>157885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>4</td>\n",
       "      <td>50577</td>\n",
       "      <td>150577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>3</td>\n",
       "      <td>87556</td>\n",
       "      <td>187556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>2</td>\n",
       "      <td>12117</td>\n",
       "      <td>112117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>10860</td>\n",
       "      <td>110860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1      x2     x3       y\n",
       "0          0  100000  15795  115795\n",
       "1          1   99999    860  100860\n",
       "2          2   99998  76820  176820\n",
       "3          3   99997  54886  154886\n",
       "4          4   99996   6265  106265\n",
       "...      ...     ...    ...     ...\n",
       "99995  99995       5  57885  157885\n",
       "99996  99996       4  50577  150577\n",
       "99997  99997       3  87556  187556\n",
       "99998  99998       2  12117  112117\n",
       "99999  99999       1  10860  110860\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fd3b69-0ad1-45ad-8dad-55158f831b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 3), (100000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.drop(columns='y').values\n",
    "y = data['y'].values\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4275d8fd-3377-4371-864b-bab5e2fbbc78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446eb1c5-1c24-4369-b310-cca3c7aa715a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "x_train = minmax_scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d5f64a-4a35-4e3f-8f75-ff5bc7fc51de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor = BatchGradientDescentRegressor(eta=1, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0566db8a-eeab-47c4-bee1-ef865b99f813",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2000\tTraining MSE: 23286462149.17448\n",
      "Epoch 2 / 2000\tTraining MSE: 13549309154.724\n",
      "Epoch 3 / 2000\tTraining MSE: 7904065834.50988\n",
      "Epoch 4 / 2000\tTraining MSE: 4628342869.60223\n",
      "Epoch 5 / 2000\tTraining MSE: 2725136965.03407\n",
      "Epoch 6 / 2000\tTraining MSE: 1617280355.64333\n",
      "Epoch 7 / 2000\tTraining MSE: 970603067.65962\n",
      "Epoch 8 / 2000\tTraining MSE: 591586841.4301\n",
      "Epoch 9 / 2000\tTraining MSE: 368130752.32659\n",
      "Epoch 10 / 2000\tTraining MSE: 235267643.90196\n",
      "Epoch 11 / 2000\tTraining MSE: 155320898.93699\n",
      "Epoch 12 / 2000\tTraining MSE: 106418677.19266\n",
      "Epoch 13 / 2000\tTraining MSE: 75845184.00972\n",
      "Epoch 14 / 2000\tTraining MSE: 56191521.87947\n",
      "Epoch 15 / 2000\tTraining MSE: 43126945.43992\n",
      "Epoch 16 / 2000\tTraining MSE: 34108058.05786\n",
      "Epoch 17 / 2000\tTraining MSE: 27630961.66758\n",
      "Epoch 18 / 2000\tTraining MSE: 22797746.68576\n",
      "Epoch 19 / 2000\tTraining MSE: 19064999.1607\n",
      "Epoch 20 / 2000\tTraining MSE: 16097731.45845\n",
      "Epoch 21 / 2000\tTraining MSE: 13684397.27206\n",
      "Epoch 22 / 2000\tTraining MSE: 11687320.25699\n",
      "Epoch 23 / 2000\tTraining MSE: 10013671.15911\n",
      "Epoch 24 / 2000\tTraining MSE: 8598383.0763\n",
      "Epoch 25 / 2000\tTraining MSE: 7394015.38649\n",
      "Epoch 26 / 2000\tTraining MSE: 6364673.29682\n",
      "Epoch 27 / 2000\tTraining MSE: 5482303.99891\n",
      "Epoch 28 / 2000\tTraining MSE: 4724393.66854\n",
      "Epoch 29 / 2000\tTraining MSE: 4072497.10502\n",
      "Epoch 30 / 2000\tTraining MSE: 3511268.15523\n",
      "Epoch 31 / 2000\tTraining MSE: 3027796.26621\n",
      "Epoch 32 / 2000\tTraining MSE: 2611134.27018\n",
      "Epoch 33 / 2000\tTraining MSE: 2251948.97347\n",
      "Epoch 34 / 2000\tTraining MSE: 1942253.28025\n",
      "Epoch 35 / 2000\tTraining MSE: 1675194.52835\n",
      "Epoch 36 / 2000\tTraining MSE: 1444883.14163\n",
      "Epoch 37 / 2000\tTraining MSE: 1246251.32963\n",
      "Epoch 38 / 2000\tTraining MSE: 1074934.96804\n",
      "Epoch 39 / 2000\tTraining MSE: 927173.89049\n",
      "Epoch 40 / 2000\tTraining MSE: 799727.14428\n",
      "Epoch 41 / 2000\tTraining MSE: 689800.62303\n",
      "Epoch 42 / 2000\tTraining MSE: 594985.06797\n",
      "Epoch 43 / 2000\tTraining MSE: 513202.83507\n",
      "Epoch 44 / 2000\tTraining MSE: 442662.12034\n",
      "Epoch 45 / 2000\tTraining MSE: 381817.55859\n",
      "Epoch 46 / 2000\tTraining MSE: 329336.2853\n",
      "Epoch 47 / 2000\tTraining MSE: 284068.69065\n",
      "Epoch 48 / 2000\tTraining MSE: 245023.20926\n",
      "Epoch 49 / 2000\tTraining MSE: 211344.5843\n",
      "Epoch 50 / 2000\tTraining MSE: 182295.12451\n",
      "Epoch 51 / 2000\tTraining MSE: 157238.54053\n",
      "Epoch 52 / 2000\tTraining MSE: 135626.00466\n",
      "Epoch 53 / 2000\tTraining MSE: 116984.12786\n",
      "Epoch 54 / 2000\tTraining MSE: 100904.5898\n",
      "Epoch 55 / 2000\tTraining MSE: 87035.19465\n",
      "Epoch 56 / 2000\tTraining MSE: 75072.1565\n",
      "Epoch 57 / 2000\tTraining MSE: 64753.44518\n",
      "Epoch 58 / 2000\tTraining MSE: 55853.04686\n",
      "Epoch 59 / 2000\tTraining MSE: 48176.0135\n",
      "Epoch 60 / 2000\tTraining MSE: 41554.19285\n",
      "Epoch 61 / 2000\tTraining MSE: 35842.54526\n",
      "Epoch 62 / 2000\tTraining MSE: 30915.96693\n",
      "Epoch 63 / 2000\tTraining MSE: 26666.54961\n",
      "Epoch 64 / 2000\tTraining MSE: 23001.21714\n",
      "Epoch 65 / 2000\tTraining MSE: 19839.68671\n",
      "Epoch 66 / 2000\tTraining MSE: 17112.71045\n",
      "Epoch 67 / 2000\tTraining MSE: 14760.55863\n",
      "Epoch 68 / 2000\tTraining MSE: 12731.71142\n",
      "Epoch 69 / 2000\tTraining MSE: 10981.73042\n",
      "Epoch 70 / 2000\tTraining MSE: 9472.28531\n",
      "Epoch 71 / 2000\tTraining MSE: 8170.31429\n",
      "Epoch 72 / 2000\tTraining MSE: 7047.29993\n",
      "Epoch 73 / 2000\tTraining MSE: 6078.64453\n",
      "Epoch 74 / 2000\tTraining MSE: 5243.13137\n",
      "Epoch 75 / 2000\tTraining MSE: 4522.45997\n",
      "Epoch 76 / 2000\tTraining MSE: 3900.84527\n",
      "Epoch 77 / 2000\tTraining MSE: 3364.67186\n",
      "Epoch 78 / 2000\tTraining MSE: 2902.19579\n",
      "Epoch 79 / 2000\tTraining MSE: 2503.28733\n",
      "Epoch 80 / 2000\tTraining MSE: 2159.20905\n",
      "Epoch 81 / 2000\tTraining MSE: 1862.42454\n",
      "Epoch 82 / 2000\tTraining MSE: 1606.43322\n",
      "Epoch 83 / 2000\tTraining MSE: 1385.62806\n",
      "Epoch 84 / 2000\tTraining MSE: 1195.17269\n",
      "Epoch 85 / 2000\tTraining MSE: 1030.89552\n",
      "Epoch 86 / 2000\tTraining MSE: 889.19835\n",
      "Epoch 87 / 2000\tTraining MSE: 766.97753\n",
      "Epoch 88 / 2000\tTraining MSE: 661.55603\n",
      "Epoch 89 / 2000\tTraining MSE: 570.62477\n",
      "Epoch 90 / 2000\tTraining MSE: 492.19207\n",
      "Epoch 91 / 2000\tTraining MSE: 424.53999\n",
      "Epoch 92 / 2000\tTraining MSE: 366.18673\n",
      "Epoch 93 / 2000\tTraining MSE: 315.85415\n",
      "Epoch 94 / 2000\tTraining MSE: 272.43982\n",
      "Epoch 95 / 2000\tTraining MSE: 234.99281\n",
      "Epoch 96 / 2000\tTraining MSE: 202.69291\n",
      "Epoch 97 / 2000\tTraining MSE: 174.83266\n",
      "Epoch 98 / 2000\tTraining MSE: 150.80181\n",
      "Epoch 99 / 2000\tTraining MSE: 130.07401\n",
      "Epoch 100 / 2000\tTraining MSE: 112.19527\n",
      "Epoch 101 / 2000\tTraining MSE: 96.77396\n",
      "Epoch 102 / 2000\tTraining MSE: 83.47233\n",
      "Epoch 103 / 2000\tTraining MSE: 71.99901\n",
      "Epoch 104 / 2000\tTraining MSE: 62.1027\n",
      "Epoch 105 / 2000\tTraining MSE: 53.56665\n",
      "Epoch 106 / 2000\tTraining MSE: 46.20388\n",
      "Epoch 107 / 2000\tTraining MSE: 39.85313\n",
      "Epoch 108 / 2000\tTraining MSE: 34.3753\n",
      "Epoch 109 / 2000\tTraining MSE: 29.65039\n",
      "Epoch 110 / 2000\tTraining MSE: 25.57493\n",
      "Epoch 111 / 2000\tTraining MSE: 22.05964\n",
      "Epoch 112 / 2000\tTraining MSE: 19.02753\n",
      "Epoch 113 / 2000\tTraining MSE: 16.41219\n",
      "Epoch 114 / 2000\tTraining MSE: 14.15632\n",
      "Epoch 115 / 2000\tTraining MSE: 12.21053\n",
      "Epoch 116 / 2000\tTraining MSE: 10.53218\n",
      "Epoch 117 / 2000\tTraining MSE: 9.08453\n",
      "Epoch 118 / 2000\tTraining MSE: 7.83586\n",
      "Epoch 119 / 2000\tTraining MSE: 6.75881\n",
      "Epoch 120 / 2000\tTraining MSE: 5.82981\n",
      "Epoch 121 / 2000\tTraining MSE: 5.0285\n",
      "Epoch 122 / 2000\tTraining MSE: 4.33733\n",
      "Epoch 123 / 2000\tTraining MSE: 3.74116\n",
      "Epoch 124 / 2000\tTraining MSE: 3.22694\n",
      "Epoch 125 / 2000\tTraining MSE: 2.78339\n",
      "Epoch 126 / 2000\tTraining MSE: 2.40081\n",
      "Epoch 127 / 2000\tTraining MSE: 2.07082\n",
      "Epoch 128 / 2000\tTraining MSE: 1.78618\n",
      "Epoch 129 / 2000\tTraining MSE: 1.54067\n",
      "Epoch 130 / 2000\tTraining MSE: 1.32891\n",
      "Epoch 131 / 2000\tTraining MSE: 1.14625\n",
      "Epoch 132 / 2000\tTraining MSE: 0.98869\n",
      "Epoch 133 / 2000\tTraining MSE: 0.8528\n",
      "Epoch 134 / 2000\tTraining MSE: 0.73558\n",
      "Epoch 135 / 2000\tTraining MSE: 0.63447\n",
      "Epoch 136 / 2000\tTraining MSE: 0.54727\n",
      "Epoch 137 / 2000\tTraining MSE: 0.47204\n",
      "Epoch 138 / 2000\tTraining MSE: 0.40716\n",
      "Epoch 139 / 2000\tTraining MSE: 0.3512\n",
      "Epoch 140 / 2000\tTraining MSE: 0.30292\n",
      "Epoch 141 / 2000\tTraining MSE: 0.26129\n",
      "Epoch 142 / 2000\tTraining MSE: 0.22537\n",
      "Epoch 143 / 2000\tTraining MSE: 0.1944\n",
      "Epoch 144 / 2000\tTraining MSE: 0.16768\n",
      "Epoch 145 / 2000\tTraining MSE: 0.14463\n",
      "Epoch 146 / 2000\tTraining MSE: 0.12475\n",
      "Epoch 147 / 2000\tTraining MSE: 0.1076\n",
      "Epoch 148 / 2000\tTraining MSE: 0.09281\n",
      "Epoch 149 / 2000\tTraining MSE: 0.08006\n",
      "Epoch 150 / 2000\tTraining MSE: 0.06905\n",
      "Epoch 151 / 2000\tTraining MSE: 0.05956\n",
      "Epoch 152 / 2000\tTraining MSE: 0.05137\n",
      "Epoch 153 / 2000\tTraining MSE: 0.04431\n",
      "Epoch 154 / 2000\tTraining MSE: 0.03822\n",
      "Epoch 155 / 2000\tTraining MSE: 0.03297\n",
      "Epoch 156 / 2000\tTraining MSE: 0.02844\n",
      "Epoch 157 / 2000\tTraining MSE: 0.02453\n",
      "Epoch 158 / 2000\tTraining MSE: 0.02116\n",
      "Epoch 159 / 2000\tTraining MSE: 0.01825\n",
      "Epoch 160 / 2000\tTraining MSE: 0.01574\n",
      "Epoch 161 / 2000\tTraining MSE: 0.01358\n",
      "Epoch 162 / 2000\tTraining MSE: 0.01171\n",
      "Epoch 163 / 2000\tTraining MSE: 0.0101\n",
      "Epoch 164 / 2000\tTraining MSE: 0.00871\n",
      "Epoch 165 / 2000\tTraining MSE: 0.00752\n",
      "Epoch 166 / 2000\tTraining MSE: 0.00648\n",
      "Epoch 167 / 2000\tTraining MSE: 0.00559\n",
      "Exiting as reduction in MSE < tol\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BatchGradientDescentRegressor(eta=1, epochs=2000, tot=0.001, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f284d2-0bdc-4de0-bc5b-78b3ebd67a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66668.07920447, 33334.0473283 , 33334.04458626, 99993.76078236])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a892e386-f2b4-4665-8792-dd6b589abee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = minmax_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc87ed3-0f15-48a6-8ebd-0844fe37dea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = regressor.predict(x_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5e2459-7b8c-410b-a933-819df77c17f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137734.03560006, 168307.96257965, 143560.02013075, ...,\n",
       "       133411.04511243, 112388.09590683, 108151.10523137])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b06b02-c0f8-4271-a2ec-d85b9f247c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137734, 168308, 143560, ..., 133411, 112388, 108151], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad1033e-a845-484c-9aee-8642b3c1f654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test MSE\n",
    "np.round(mean_squared_error(y_test, pred), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38180b12-25ff-4920-b912-ea29e15b8f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
